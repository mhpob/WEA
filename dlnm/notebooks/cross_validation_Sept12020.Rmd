---
title: "Species abundance cross validation"
subtitle: "Take 2: slight model adjustments"
author: "Mike O'Brien"
date: "09/01/2020"
output:
  # pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# knitr::opts_knit$set(root.dir = 'c:/users/darpa2/analysis/wea-analysis/dlnm')
knitr::opts_knit$set(root.dir = 'p:/obrien/biotelemetry/md wea habitat/wea-analysis/dlnm')
```

## Updates
There are a few differences between this document and the one produced on July 31, 2020. 

- All combinations of models are now being considered.
- The smooth interaction term for $SSTlag$ now uses 36 ($6^2$) knots rather than 100 ($10^2$). This brings it in line with the other smooths (only using 6 knots).
- Tensor product smooths have been specified by `te()` rahter than `t2()`.
- Tensor product smooths are now explicitly using 36 ($6^2$) knots.

## Packages
```{r}
library(parallel); library(dlnm); library(mgcv); library(dplyr)
```

## Custom functions
```{r}
cv <- function(data, model, k, repeats = 1, cl = NULL){
  create_input_data <- function(model_data){
    data_list <- as.list(model_data[, !grepl('SST.', names(model_data))])
    
    Q_species <- grep('^Q', names(model$var.summary), value = T)
    L_species <- grep('^L', names(model$var.summary), value = T)
    
    if(length(Q_species) == 1){
      # Set up SST lag matrix, and assign to an element of the list
      data_list[[Q_species]] <- 
        as.matrix(
          subset(model_data, select = grep('SST.', names(model_data), value = T))
        )
      
      # Define lags, and assign to an element of the list
      data_list[[L_species]] <- 
        matrix(0:(ncol(data_list[[Q_species]]) - 1),
               nrow(data_list[[Q_species]]),
               ncol(data_list[[Q_species]]),
               byrow = TRUE)
    }
    
    data_list
  }
  
  refit <- function(i, data_shuffle, folds){
    # Segment data by fold using the which() function
    test_ids <- which(folds == i, arr.ind = TRUE)
    test_data <- data_shuffle[test_ids, ]
    train_data <- data_shuffle[-test_ids, ]
    
    
    # Train the model
    train_data <- create_input_data(train_data)
    
    CV_mod <- gam(data = train_data,
                  formula = model$formula,
                  family = ziP,
                  method = 'REML')
    
    train_data$pred <- predict(CV_mod, type = 'response')
    
    
    # Test the model
    test_data <- create_input_data(test_data)
    test_data$pred <- predict(CV_mod, test_data, type = "response")
    
    
    
    # Penalty functions
    ## Overall RMSE
    train_overall <- sqrt(mean((train_data$freq - train_data$pred) ^ 2))
    test_overall <- sqrt(mean((test_data$freq - test_data$pred) ^ 2))
    
    ## Zero-only RMSE
    train_0 <- sqrt(mean((train_data$freq[train_data$freq == 0] -
                            train_data$pred[train_data$freq == 0]) ^ 2))
    test_0 <- sqrt(mean((test_data$freq[test_data$freq == 0] -
                           test_data$pred[test_data$freq == 0]) ^ 2))
    
    ## Greater-than-zero RMSE
    train_gt0 <- sqrt(mean((train_data$freq[train_data$freq != 0] -
                              train_data$pred[train_data$freq != 0]) ^ 2))
    test_gt0 <- sqrt(mean((test_data$freq[test_data$freq != 0] -
                             test_data$pred[test_data$freq != 0]) ^ 2))
    
    
    c(train_overall = train_overall, test_overall = test_overall,
      train_0 = train_0, test_0 = test_0,
      train_gt0 = train_gt0, test_gt0 = test_gt0)
  }
  
  
  if(repeats != 1){
    reps <- function(reps){
      
      data_shuffle <- data[sample(nrow(data)),]
      folds <- cut(seq(1, nrow(data_shuffle)), breaks = k, labels = F)
      
      if(is.null(cl)){
        folds <- sapply(1:k, refit, data_shuffle, folds)
      }else{
        clusterExport(cl, list('data_shuffle', 'folds'))
        folds <- parSapply(cl, 1:k, refit, data_shuffle, folds)
      }
      
      data.frame((t(folds)),
                 fold = 1:k,
                 rep = reps)
    }
    
    
    if(is.null(cl)){
      cvs <- lapply(1:repeats, reps)
    }else{
      parallel::clusterEvalQ(cl, c(library(dlnm), library(mgcv)))
      cvs <- parLapply(cl, 1:repeats, reps)
    }
    
    
    do.call(rbind, cvs)
    
  }else{
    
    data_shuffle <- data[sample(nrow(data)),]
    folds <- cut(seq(1, nrow(data_shuffle)), breaks = k, labels = F)
    
    if(is.null(cl)){
      folds <- sapply(1:k, refit, data_shuffle, folds)
    }else{
      clusterEvalQ(cl, c(library(dlnm), library(mgcv)))
      # clusterExport(cl, c('data_shuffle', 'folds'))
      folds <- parSapply(cl, 1:k, refit, data_shuffle, folds)
    }
    
    data.frame((t(folds)),
               fold = 1:k)
    
  }
}
```
## Import data
### Sturgeon
```{r}
sturg <- read.csv("data/sturg_w_lags.csv")
sturg$CHLA <- log(sturg$CHLA)
sturg$Site <- as.factor(sturg$Site)
head(sturg)
```

### Striped bass
```{r}
sb <- read.csv("data/bass_w_lags.csv")
sb$CHLA <- log(sb$CHLA)
sb$Site <- as.factor(sb$Site)
head(sb)
```


## Model selection

The manuscript only reports the top five models for each species, so I'm only going to focus on them. Models have already been fitted, so I'm just importing them.

```{r}
as_mods <- readRDS('sturg_lag_models.RDS')
as_aic <- data.frame(model = names(as_mods),
                     AIC = sapply(as_mods, AIC)) %>% 
  arrange(AIC) %>% 
  mutate(dAIC = AIC - min(AIC),
         wAIC = exp(-0.5 * dAIC),
         wAIC = wAIC / sum(wAIC),
         cwAIC = cumsum(wAIC))


sb_mods <- readRDS('sb_lag_models.rds')
sb_aic <- data.frame(model = names(sb_mods),
                     AIC = sapply(sb_mods, AIC)) %>% 
  arrange(AIC) %>% 
  mutate(dAIC = AIC - min(AIC),
         wAIC = exp(-0.5 * dAIC),
         wAIC = wAIC / sum(wAIC),
         cwAIC = cumsum(wAIC))


as_aic[order(as_aic$AIC),][1:5,]
sb_aic[order(sb_aic$AIC),][1:5,]

```

Models 7, 6, 8, 9, and 10 are the top five for Atlantic sturgeon. Model 7 has the most support, with 83% of the weight, though there is also some support for model 6, with 17% of the weight. For striped bass, models 6, 7, 11, 10, and 8 are the top five. According to AIC weights, only model 6 has support.

```{r}
as_mods <- as_mods[names(as_mods) %in% paste0('SM', c(7, 6, 8, 9, 10))]
sb_mods <- sb_mods[names(sb_mods) %in% paste0('BM', c(6, 7, 11, 10, 8))]
```

## Cross validation
### Atlantic sturgeon

The best models, in order, were 7, 6, 8, 9, and 10 for Atlantic sturgeon. Calculate 5-fold cross validation in parallel. This is not run here, as it takes too long -- the saved output was imported above.
```{r, eval=FALSE}
# Create cluster to run models in parallel
parallel_cluster <- makeCluster(detectCores(logical = F) - 1)
clusterExport(parallel_cluster, 'as_mods')

# Perform 5-fold cross validation in parallel
kf_SM7 <- cv(sturg, as_mods$SM7, 5, cl = parallel_cluster)
kf_SM6 <- cv(sturg, as_mods$SM6, 5, cl = parallel_cluster)
kf_SM8 <- cv(sturg, as_mods$SM8, 5, cl = parallel_cluster)
kf_SM9 <- cv(sturg, as_mods$SM9, 5, cl = parallel_cluster)
kf_SM10 <- cv(sturg, as_mods$SM10, 5, cl = parallel_cluster)


# Close cluster
stopCluster(parallel_cluster)

as_kfcv <- list(kf_SM7 = kf_SM7,
                kf_SM6 = kf_SM6,
                kf_SM8 = kf_SM8,
                kf_SM9 = kf_SM9,
                kf_SM10 = kf_SM10)

saveRDS(as_kfcv, 'data/sturgeon_5fcv.rds')

```

Produce mean RMSE and standard deviation of the RMSE for the predictions.

```{r}
as_kfcv <- readRDS('data/sturgeon_5fcv.rds')

# RMSE mean
t(sapply(as_kfcv, function(.) colMeans(.[,1:6])))

# RMSE SD
t(sapply(as_kfcv, function(.) apply(.[, 1:6], 2, sd)))
```


#### RMSE interpretation
1. When predicting true 0 counts, the models tended to overestimate by 0.13 individuals, or 13.4-13.6%. When predicting incidence, the models were off by 1.14-1.16 individuals, or 22.8-23.2% of the maximum value of incidence (max = 5).

2. The RMSE of the test set tends to be a little bit larger than the training set, which indicates some over-fitting, but they're pretty close. I'd say the models aren't appreciably over- or under-fitting new data.

3. SM7 ($\sim SSTlag + DOY:Depth$) was the best model according to AIC, with some support for SM6 ($\sim SSTlag + log(CHLA) + DOY:Depth$). These models perform nearly identically in cross validation, so SM7 should be selected for parsimony's sake.


### Striped bass

The top-5 models, in order, were 6, 7, 11, 10, and 8 for striped bass. No model had support after model 6. Calculate 5-fold cross validation in parallel. This is not run here, as it takes too long -- the saved output was imported above.

```{r, eval=FALSE}
# Create cluster to run models in parallel
parallel_cluster <- makeCluster(detectCores(logical = F) - 1)
clusterExport(parallel_cluster, 'sb_mods')

# Perform 5-fold cross validation in parallel
kf_BM6 <- cv(sb, sb_mods$BM6, 5, cl = parallel_cluster)
kf_BM7 <- cv(sb, sb_mods$BM7, 5, cl = parallel_cluster)
kf_BM11 <- cv(sb, sb_mods$BM11, 5, cl = parallel_cluster)
kf_BM10 <- cv(sb, sb_mods$BM10, 5, cl = parallel_cluster)
kf_BM8 <- cv(sb, sb_mods$BM8, 5, cl = parallel_cluster)



# Close cluster
stopCluster(parallel_cluster)

sb_kfcv <- list(kf_BM6 = kf_BM6,
                kf_BM7 = kf_BM7,
                kf_BM11 = kf_BM11,
                kf_BM10 = kf_BM10,
                kf_BM8 = kf_BM8)

saveRDS(sb_kfcv, 'data/bass_5fcv.rds')
```

Produce mean RMSE and Standard deviation of the RMSE for the predictions.

```{r}
sb_kfcv <- readRDS('data/bass_5fcv.rds')

# RMSE mean
t(sapply(sb_kfcv, function(.) colMeans(.[,1:6])))

# RMSE SD
t(sapply(sb_kfcv, function(.) apply(.[, 1:6], 2, sd)))
```

#### RMSE interpretation
1. When predicting true 0 counts, the models tended to overestimate by 0.36 individuals, or 35-37%. When predicting incidence, the models were off by ~3 individuals, or 15.9-16.8% of the maximum value of incidence (max = 19).

2) Models don't show too much overfitting.

3) BM6 ($\sim SSTlag + log(CHLA) + DOY:Depth$), the lowest AIC model, had slightly more prediction error than the next three-best models. However, it clearly fit the data best --this is possibly something to note in the discussion.

## Overall
One thing that the RMSE should allow us to do is compare models across species if comparing like-to-like. In this instance, we can *definitely* compare performance of predicting zeroes, and *possibly* compare performance in reference to maximum value. Here, the sturgeon models were able to more-accurately predict absence (13% vs 36% error), but not count (23% vs 16% error). This may mean that, comparatively, zero counts influence the model more for sturgeon, while positive counts influence the model more for striped bass.