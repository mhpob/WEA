---
title: "Slices and model visualization"
author: "Mike O'Brien"
date: "Sept. 21, 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = 'c:/users/darpa2/analysis/wea-analysis/dlnm')
# knitr::opts_knit$set(root.dir = 'p:/obrien/biotelemetry/md wea habitat/wea-analysis/dlnm')
```

## Packages
```{r}
library(dlnm)
library(mgcv)
library(ggplot2)
library(tidyr)
```


## Pull in winning models
Sturgeon model 7...

$\sim \beta_0 + f(SSTlag) + f(DOY:Depth) + \beta_{1_j} + \beta_{2_i} + ln(D_{50)}$

...and striped bass model 6...

$\sim \beta_0 + f(SSTlag) + f(ln(CHLA)) + f(DOY:Depth) + \beta_{1_j} + \beta_{2_i} + ln(D_{50})$

...were selected as the best models via AIC; $\beta_0$ is the intercept, $\beta_{1_j}$ is the random effect of the $j^{th}$ site, $\beta_{2_i}$ is the random effect of the $i^{th}$ year, and $D_{50}$ is the 50% probability of detection.

```{r}
s_mod <- readRDS('sturg_lag_models.RDS')[['SM7']]
b_mod <- readRDS('sb_lag_models.rds')[['BM6']]
```


## Lag plots
### Sturgeon

Using the built-in contour plotting from `dlnm`, we can get something like the below, which I think makes it easier to see what's going on than a 3D plot.

```{r}
plot(crosspred('Qs2', s_mod, cen = 0), ptype = 'contour')
```

Unfortunately, this doesn't give us standard errors unless we switch over to taking slices along a lag...

```{r}
# Pick lags of 0 and 18 since that's where something seems to be happening
plot(crosspred('Qs2', s_mod, cen = 0),
     ptype = 'slices', lag = c(0, 18))
```

...and the response is always reported in reference to what the predicted outcome at 0 lag was (the `cen = 0` in the code above). So, the predicted value will always be 0 at the lag we chose with `cen = 0`. I'm going to try and recreate this using `predict.gam`, allowing us to not use the 0 reference value and get a clearer view of what this is doing to the model.

```{r}
# Grid of SSTlag (Qs2) and lag values (Ls2) over which to predict
new_data_lag <- expand.grid(
  Qs2 = seq(-9, 11,length.out = 100),
  Ls2 = seq(0, 29, length.out = 100)
)

# assume constant median offset for now
#   need to take the exponent of this, as the log will be taken during the
#     prediction process
new_data_lag$d50.s <- exp(median(s_mod$model$`offset(log(d50.s))`))

# predict on link scale, setting the effects of the DOY-depth interaction and
#   random site and year to zero ("exclude = ...")
# "newdate.guaranteed = T" just allows me to not include dummy DOY, Depth, Site,
#   and Year values that will be removed anyway
preds <- predict(s_mod, new_data_lag, type = 'link',
                    exclude = c('te(DOY,Depth)', 's(Site)', 's(Year)'),
                    newdata.guaranteed = T)

new_data_lag$pred <- preds
```

To recreate the contour plot above, first plot out the predictions on the link scale.

```{r}
ggplot() +
  geom_contour_filled(data = new_data_lag, aes(x = Qs2, y = Ls2, z = pred)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_minimal()
```

These look somewhat similar, but they are almost 4 units less than the predictions reported in the other plot. At a Qs2 (SSTlag) value of 0, it looks like the predictions are around -3. So, lets try to add 3 to the predictions to "center" it around SSTlag = 0.

```{r}
ggplot(data = new_data_lag, aes(x = Qs2, y = Ls2, z = pred + 3)) +
  geom_contour_filled() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_minimal()
```

Yes, that's pretty darn similar for a shot in the dark. Whereas we chose one centering value and applied it globally, across the plot, I believe that `crosspred` centers each lag individually. 

#### WARNING: TANGENT
Did you ever notice how, in the basic contour plots, `mgcv` will often have areas where it plots nothing? You can see what I mean here:

```{r}
plot(s_mod, select = 4)
```

In the DOY:Depth plot above, there is a strip at a depth of ~33 meters where nothing is plotted. This is because `mgcv` removes observations >10% of the range away from an observation in any direction. Here, that means that there were no observations within 10% of the range of observed depths from 33 meters. Splines can go off in all sorts of crazy directions, especially when they're unconstrained -- which usually happens around the edges of the plot.

So, in order to avoid trying to interpret an artifact that is a result of an unconstrained spline, I'm going to copy `mgcv` and remove predictions that are farther than 10% of the range of observations. Unfortunately, this doesn't really help when looking at a normal spline, so for those I'll just show a rug.

#### Back to regularly-scheduled programming

Now, what does the contour plot look like on the response scale? Not going to center on 0 here.

```{r}
new_data_lag <- expand.grid(
  Qs2 = seq(-9, 11, length.out = 100),
  Ls2 = seq(0, 29, length.out = 100)
)

# assume constant offset for now
new_data_lag$d50.s <- exp(median(s_mod$model$`offset(log(d50.s))`))

# remove predictions too far from an observation
too_far <- exclude.too.far(new_data_lag$Qs2, new_data_lag$Ls2,
                           s_mod$model$Qs2, s_mod$model$Ls2, 0.1)
new_data_lag <- new_data_lag[!too_far,]

preds <- predict(s_mod, new_data_lag, type = 'response',
                    exclude = c('s(Site)', 's(Year)', 'te(DOY,Depth)'),
                    newdata.guaranteed = T)

new_data_lag$pred <- preds

ggplot(data = new_data_lag, aes(x = Qs2, y = Ls2, z = pred)) +
  geom_contour_filled() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_minimal()
```

What about taking slices?

```{r}
new_data_lag <- expand.grid(
  Qs2 = seq(-9, 11, length.out = 100),
  Ls2 = seq(0, 29, 1)
)

new_data_lag <- data.frame(Qs2 = new_data_lag$Qs2,
                           Ls2 = new_data_lag$Ls2,
                           d50.s = exp(
                             mean(s_mod$model$`offset(log(d50.s))`)
                           )
)

preds <- predict(s_mod, new_data_lag, type = 'link', se = T,
              exclude = c('s(Site)', 's(Year)', 'te(DOY,Depth)'),
              newdata.guaranteed = T)
```

Quick note: for calculation and convergence purposes, `mgcv` uses the identity link when using the `ziP` family. However, results on the link scale, which should be the same as the response scale due to the identity link, are actually returned on the log scale. Because of this, I can't use a baked-in inverse link function; I'm just going to take the exponent of the returned link values. If you feel like playing around with it, you can see that results from `mgcv::predict(..., type = 'response')` will match `exp( mgcv::predict(..., type = 'link') )`.

```{r}
# Calculate predictions and confidence intervals
new_data_lag$pred <- exp(preds$fit)
new_data_lag$lci <- exp(preds$fit - 1.96 * preds$se.fit)
new_data_lag$uci <- exp(preds$fit + 1.96 * preds$se.fit)


# Find observed values to make a rug
obs <- data.frame(s_mod$model$Qs2)
obs <- pivot_longer(obs, cols = starts_with('SST'),
                    names_to = 'Ls2', values_to = 'Qs2')

# convert lag written as "SST*" to a number
obs$Ls2 <- as.numeric(gsub('SST', '', obs$Ls2)) - 1

# drop repeated combinations
obs <- unique(obs[, c('Ls2', 'Qs2')])


slicer <- function(a){
  ggplot() +
    geom_ribbon(data = new_data_lag[new_data_lag$Ls2 %in% seq(a, a + 3, 1),],
                aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
                fill = 'lightgray') +
    geom_line(data = new_data_lag[new_data_lag$Ls2 %in% seq(a, a + 3, 1),],
              aes(x = Qs2, y = pred)) +
    geom_rug(data = obs[obs$Ls2 %in% seq(a, a + 3, 1),],
             aes(x = Qs2), alpha = 0.2, sides = 't') +
    coord_cartesian(ylim = c(0, 2)) +
    scale_x_continuous(expand = c(0, 0)) +
    facet_wrap(~Ls2)+
    theme_minimal()
}

lapply(seq(0, 29, by = 4), slicer)

```
These aren't very engaging. However, this is the marginal effect of the SST lag smooth (partial effect? I always forget...). This means that effects of all other variables were dropped from the model. So $\sim f(SSTlag) + f(DOY:Depth)$, in essence, becomes $\sim f(SSTlag) + 0 * f(DOY:Depth)$ (that explanation is likely incorrect, but that's how I see it in my non-statistician mind). What if we focus on the effect of $f(SSTlag)$ at different levels of $f(DOY:Depth)$? So, adjust the "intercept" up and down according to the outcome of $f(DOY:Depth)$. What would an interesting combination of DOY and depth be? I've highlighted two below:

```{r}
plot(s_mod, select = 4)
points(125, 16, col = 'blue', pch = 19, cex = 2)
points(280, 29, col = 'blue', pch = 19, cex = 2)
```

Okay, so it looks like a depth of 16m on day 125 (May 4) and a depth of 29m on day 280 (Oct 6) are predicted sturgeon hot spots (if all other model terms are set to zero). How does the predicted number of sturgeon respond on those days/depths to lagged SST?

```{r}
new_data_lag <- expand.grid(
  Qs2 = seq(-9, 11,length.out = 100),
  Ls2 = seq(0, 29, length.out = 100)
)
new_data_lag <- data.frame(Qs2 = rep(new_data_lag$Qs2, times = 2),
                           Ls2 = rep(new_data_lag$Ls2, times = 2),
                           DOY = rep(c(125, 280), times = nrow(new_data_lag)),
                           Depth = rep(c(16, 29), times = nrow(new_data_lag)),
                           d50.s = exp(rep(c(
                             # mean D50 on DOY 125
                             mean(s_mod$model[s_mod$model$DOY == 125,]$`offset(log(d50.s))`),
                             # mean D50 on DOY 280
                             mean(s_mod$model[s_mod$model$DOY == 280,]$`offset(log(d50.s))`)
                           ), times = nrow(new_data_lag)))
)

too_far <- exclude.too.far(new_data_lag$Qs2, new_data_lag$Ls2,
                           s_mod$model$Qs2, s_mod$model$Ls2, 0.1)

new_data_lag <- new_data_lag[!too_far,]

preds <- predict(s_mod, new_data_lag, type = 'response',
              exclude = c('s(Site)', 's(Year)'),
              newdata.guaranteed = T)

new_data_lag$pred <- preds

ggplot(data = new_data_lag, aes(x = Qs2, y = Ls2, z = pred)) +
  geom_contour_filled(breaks = c(seq(0, 10, 1), Inf)) +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  facet_wrap(~interaction(DOY, Depth)) +
  theme_minimal()
```

Let's look across the lags of 0, 5 (hopefully there's more observations there), 18, 26, and 29 to check the errors. Note again that the predictions reported on the link scale are the natural log of those on the response scale, so we'll take their exponent.

```{r}
new_data_lag <- expand.grid(
  Qs2 = seq(-9, 11, length.out = 100),
  Ls2 = seq(0, 29, 1)
)

new_data_lag <- data.frame(Qs2 = rep(new_data_lag$Qs2, times = 2),
                           Ls2 = rep(new_data_lag$Ls2, times = 2),
                           DOY = rep(c(125, 280), times = nrow(new_data_lag)),
                           Depth = rep(c(16, 29), times = nrow(new_data_lag)),
                           d50.s = exp(rep(c(
                             mean(s_mod$model[s_mod$model$DOY == 125,]$`offset(log(d50.s))`),
                             mean(s_mod$model[s_mod$model$DOY == 280,]$`offset(log(d50.s))`)
                           ), times = nrow(new_data_lag)))
)

preds <- predict(s_mod, new_data_lag, type = 'link', se = T,
              exclude = c('s(Site)', 's(Year)'),
              newdata.guaranteed = T)

new_data_lag$pred <- exp(preds$fit)
new_data_lag$lci <- exp(preds$fit - 1.96 * preds$se.fit)
new_data_lag$uci <- exp(preds$fit + 1.96 * preds$se.fit)


ggplot() +
  geom_ribbon(data = new_data_lag[new_data_lag$Ls2 %in% seq(0, 4),],
              aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
              fill = 'lightgray') +
  geom_line(data = new_data_lag[new_data_lag$Ls2 %in% seq(0, 4),], aes(x = Qs2, y = pred)) +
  geom_rug(data = obs[obs$Ls2 %in% seq(0, 4),],
           aes(x = Qs2), alpha = 0.2, sides = 't') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(Ls2 ~ interaction(DOY, Depth), scales = 'free_y')+
  theme_minimal()

ggplot() +
  geom_ribbon(data = new_data_lag[new_data_lag$Ls2 %in% seq(5, 9),],
              aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
              fill = 'lightgray') +
  geom_line(data = new_data_lag[new_data_lag$Ls2 %in% seq(5, 9),], aes(x = Qs2, y = pred)) +
  geom_rug(data = obs[obs$Ls2 %in% seq(5, 9),],
           aes(x = Qs2), alpha = 0.2, sides = 't') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(Ls2 ~ interaction(DOY, Depth), scales = 'free_y')+
  theme_minimal()

ggplot() +
  geom_ribbon(data = new_data_lag[new_data_lag$Ls2 %in% seq(10, 14),],
              aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
              fill = 'lightgray') +
  geom_line(data = new_data_lag[new_data_lag$Ls2 %in% seq(10, 14),], aes(x = Qs2, y = pred)) +
  geom_rug(data = obs[obs$Ls2 %in% seq(10, 14),],
           aes(x = Qs2), alpha = 0.2, sides = 't') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(Ls2 ~ interaction(DOY, Depth), scales = 'free_y')+
  theme_minimal()

ggplot() +
  geom_ribbon(data = new_data_lag[new_data_lag$Ls2 %in% seq(15, 19),],
              aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
              fill = 'lightgray') +
  geom_line(data = new_data_lag[new_data_lag$Ls2 %in% seq(15, 19),], aes(x = Qs2, y = pred)) +
  geom_rug(data = obs[obs$Ls2 %in% seq(15, 19),],
           aes(x = Qs2), alpha = 0.2, sides = 't') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(Ls2 ~ interaction(DOY, Depth), scales = 'free_y')+
  theme_minimal()

ggplot() +
  geom_ribbon(data = new_data_lag[new_data_lag$Ls2 %in% seq(20, 24),],
              aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
              fill = 'lightgray') +
  geom_line(data = new_data_lag[new_data_lag$Ls2 %in% seq(20, 24),], aes(x = Qs2, y = pred)) +
  geom_rug(data = obs[obs$Ls2 %in% seq(20, 24),],
           aes(x = Qs2), alpha = 0.2, sides = 't') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(Ls2 ~ interaction(DOY, Depth), scales = 'free_y')+
  theme_minimal()


ggplot() +
  geom_ribbon(data = new_data_lag[new_data_lag$Ls2 %in% seq(25, 29),],
              aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
              fill = 'lightgray') +
  geom_line(data = new_data_lag[new_data_lag$Ls2 %in% seq(25, 29),], aes(x = Qs2, y = pred)) +
  geom_rug(data = obs[obs$Ls2 %in% seq(25, 29),],
           aes(x = Qs2), alpha = 0.2, sides = 't') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(Ls2 ~ interaction(DOY, Depth), scales = 'free_y')+
  theme_minimal()
```

Eesh, that first CI at 0 lag is wild. Let's zoom in.

```{r}
ggplot() +
  geom_ribbon(data =  new_data_lag[new_data_lag$Ls2 == 0,],
              aes(x = Qs2, y = pred, ymin = lci, ymax = uci),
              fill = 'lightgray') +
  geom_line(data =  new_data_lag[new_data_lag$Ls2 == 0,], aes(x = Qs2, y = pred)) +
  geom_rug(data = obs[obs$Ls2 == 0,],
           aes(x = Qs2), alpha = 0.2, sides = 't') +
  coord_cartesian(ylim = c(0, 30)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(~ interaction(DOY, Depth))+
  theme_minimal()
```

We're still definitely predicting the presence of sturgeon (lower CI > 0), but we should be cautious at $SSTlag$ > 5ish since we don't have any observations out there and it quickly starts to report way more sturgeon than we recorded.

Unfortunately, there isn't much interesting here. It might be helpful if we allow an increase in the number of knots. Perhaps that could let some local minima/maxima appear? We should probably think about plowing ahead and I'll run those models in the meantime.




### Striped bass

Not to repeat the long-form explanation/analysis, I'm just going to run through the striped bass in a more rapid-fire fashion. Here's the `dlnm` contour plot:

```{r}
plot(crosspred('Qb2', b_mod, cen = 0), ptype = 'contour')
```

Based on this, I'm going to look at lags of 0, 13, nand 28.

```{r}
plot(crosspred('Qb2', b_mod, cen = 0),
     ptype = 'slices', lag = c(0, 13, 28))
```

Yep, looks like there's some positive influence at a lag of 0 and negative influence at lags of 12-16ish. Let's jump straight to the non-centered contour plot on the link scale.

```{r}
new_data_lag <- expand.grid(
  Qb2 = seq(-7, 10,length.out = 100),
  Lb2 = seq(0, 29, length.out = 100)
)

# assume constant offset for now
new_data_lag$d50.b <- exp(median(b_mod$model$`offset(log(d50.b))`))

too_far <- exclude.too.far(new_data_lag$Qb2, new_data_lag$Lb2,
                           b_mod$model$Qb2, b_mod$model$Lb2, 0.1)

new_data_lag <- new_data_lag[!too_far,]

preds <- predict(b_mod, new_data_lag, type = 'link',
                    exclude = c('s(Site)', 's(Year)', 's(CHLA)', 'te(DOY,Depth)'),
                    newdata.guaranteed = T)

new_data_lag$pred <- preds

ggplot(data = new_data_lag, aes(x = Qb2, y = Lb2, z = pred)) +
  geom_contour_filled() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_minimal()
```

A bunch of negative values. Doesn't bode well -- we're likely just chasing zeroes. Let's look at the response scale:

```{r}
new_data_lag <- expand.grid(
  Qb2 = seq(-7, 10,length.out = 100),
  Lb2 = seq(0, 29, length.out = 100)
)

# assume constant offset for now
new_data_lag$d50.b <- exp(median(b_mod$model$`offset(log(d50.b))`))

too_far <- exclude.too.far(new_data_lag$Qb2, new_data_lag$Lb2,
                           b_mod$model$Qb2, b_mod$model$Lb2, 0.1)

new_data_lag <- new_data_lag[!too_far,]

preds <- predict(b_mod, new_data_lag, type = 'response',
                    exclude = c('s(Site)', 's(Year)', 's(CHLA)', 'te(DOY,Depth)'),
                    newdata.guaranteed = T)

new_data_lag$pred <- preds

ggplot(data = new_data_lag, aes(x = Qb2, y = Lb2, z = pred)) +
  geom_contour_filled() +
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_minimal()
```

Yeah, we're basically predicting zero striped bass across the board. And the previously-noted lags of 0, 13, and 28?

```{r}
new_data_lag <- expand.grid(
  Qb2 = seq(-7, 10,length.out = 100),
  Lb2 = c(0, 13, 28)
)

new_data_lag <- data.frame(Qb2 = new_data_lag$Qb2,
                           Lb2 = new_data_lag$Lb2,
                           d50.b = exp(
                             mean(b_mod$model$`offset(log(d50.b))`)
                           )
)

preds <- predict(b_mod, new_data_lag, type = 'link', se = T,
              exclude = c('s(Site)', 's(Year)', 's(CHLA)', 'te(DOY,Depth)'),
              newdata.guaranteed = T)

# Find observed values to make a rug
obs <- data.frame(b_mod$model$Qb2)
obs <- pivot_longer(obs, cols = starts_with('SST'),
                    names_to = 'Lb2', values_to = 'Qb2')

# convert lag written as "SST*" to a number
obs$Lb2 <- as.numeric(gsub('SST', '', obs$Lb2)) - 1

# drop repeated combinations
obs <- unique(obs[, c('Lb2', 'Qb2')])


new_data_lag$pred <- exp(preds$fit)
new_data_lag$lci <- exp(preds$fit - 1.96 * preds$se.fit)
new_data_lag$uci <- exp(preds$fit + 1.96 * preds$se.fit)


ggplot() +
  geom_ribbon(data = new_data_lag,
              aes(x = Qb2, y = pred, ymin = lci, ymax = uci), fill = 'lightgray') +
  geom_line(data = new_data_lag,
              aes(x = Qb2, y = pred)) +
  geom_rug(data = obs[obs$Lb2 %in% c(0, 13, 28),],
           aes(x = Qb2), alpha = 0.2, sides = 't') +
  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +
  scale_x_continuous(expand = c(0, 0)) +
  facet_wrap(~Lb2, scales = 'free_y')+
  theme_minimal()
```
